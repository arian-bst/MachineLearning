{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1 align=\"center\">An Introduction to Machine Learning - 25737</h1>\n",
        "<h4 align=\"center\">Dr. Yasaei</h4>\n",
        "<h4 align=\"center\">Sharif University of Technology, Autumn 2024</h4>\n",
        "\n",
        "**Student Name**: Arian Bastani\n",
        "\n",
        "**Student ID**: 400100073"
      ],
      "metadata": {
        "id": "UUVoHJM1Xed5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gaussian Mixture Models with EM\n",
        "\n",
        "## Introduction and Purpose\n",
        "\n",
        "In this exercise, you will:\n",
        "\n",
        "1. Implement a **Gaussian Mixture Model (GMM)** using the Expectation-Maximization (EM) algorithm **from scratch** (using NumPy and basic Python operations).\n",
        "2. Implement the **same GMM model using PyTorch**.\n",
        "3. Compare and contrast the two implementations (performance, complexity, ease of coding, etc.).\n",
        "\n",
        "**Gaussian Mixture Models** assume that data is generated from a mixture of several Gaussian distributions. The EM algorithm iteratively updates the parameters (means, covariances, and mixture weights) of these Gaussians to maximize the likelihood of observed data.\n",
        "\n"
      ],
      "metadata": {
        "id": "_q3lWTPWu_Wx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Data Loading and Exploration\n",
        "\n",
        "**Tasks:**  \n",
        "- Load the Iris dataset and store the features in `X` and labels in `y`.\n",
        "- Print the shape of `X` and examine a few rows.\n",
        "- **Hint:** Use `sklearn.datasets.load_iris()` to load the data."
      ],
      "metadata": {
        "id": "R7YUHxtMvPOm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTjogaS8u-Sf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a135b0f1-aa6d-442e-a06c-67c514de5b24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X: (150, 4)\n",
            "First 5 samples:\n",
            " [[5.1 3.5 1.4 0.2]\n",
            " [4.9 3.  1.4 0.2]\n",
            " [4.7 3.2 1.3 0.2]\n",
            " [4.6 3.1 1.5 0.2]\n",
            " [5.  3.6 1.4 0.2]]\n"
          ]
        }
      ],
      "source": [
        "# TODO: Load the Iris dataset and print shape\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "print(\"Shape of X:\", X.shape)\n",
        "print(\"First 5 samples:\\n\", X[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Data Preprocessing (Scaling)\n",
        "\n",
        "**Tasks:**  \n",
        "- Scale the data using `StandardScaler` so that each feature has zero mean and unit variance.\n",
        "- **Hint:** `from sklearn.preprocessing import StandardScaler`.\n"
      ],
      "metadata": {
        "id": "5MxjH7L4vVVd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Scale the data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "print(\"Mean after scaling:\", X_scaled.mean(axis=0))\n",
        "print(\"Std after scaling:\", X_scaled.std(axis=0))\n"
      ],
      "metadata": {
        "id": "kIxnM1ybvSpg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "befe357d-f976-4e6a-aca1-fc18cfbdf037"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean after scaling: [-1.69031455e-15 -1.84297022e-15 -1.69864123e-15 -1.40924309e-15]\n",
            "Std after scaling: [1. 1. 1. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Implementing GMM with EM **from scratch** (NumPy-based)\n",
        "\n",
        "We will first implement GMM using NumPy arrays and basic operations, without PyTorch.\n",
        "\n",
        "**Tasks:**  \n",
        "- Choose the number of components `K` (e.g., K=3).\n",
        "- Initialize the parameters: means, covariances (diagonal), and mixture weights.\n",
        "- Write functions for the E-step and M-step of the EM algorithm.\n",
        "- Run the EM algorithm for a fixed number of iterations.\n",
        "\n",
        "**Hints for Implementation:**\n",
        "\n",
        "- Means: K x D array.\n",
        "- Covariances: K x D x D (diagonal only, so you mainly store variances per feature).\n",
        "- Weights: K-dimensional array, summing to 1.\n",
        "- To compute Gaussian densities, recall the formula for the probability density of a multivariate Gaussian.\n",
        "- For the E-step, compute responsibilities using the mixture components and their densities.\n",
        "- For the M-step, update means, covariances, and weights using the responsibilities.\n",
        "\n",
        "After implementing and running EM, extract cluster assignments by taking `argmax` of responsibilities.\n"
      ],
      "metadata": {
        "id": "QlrQIMl7vYde"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Set number of components\n",
        "K = 3\n",
        "N, D = X_scaled.shape\n",
        "\n",
        "# TODO: Initialize means, covariances, and weights\n",
        "means = np.random.rand(K, D)\n",
        "covariances = 10*np.ones((K, D))  # Diagonal covariances, so you can store just var per component and construct diag\n",
        "weights = np.random.rand(K)\n",
        "weights /= weights.sum()\n",
        "\n",
        "# TODO: Define a function to compute Gaussian PDF values for each component\n",
        "def gaussian_pdf(X, mean, cov):\n",
        "    P = (X-mean)**2 / cov.diagonal() / 2\n",
        "    return np.exp(-P.sum(axis=1)) / np.sqrt(cov.diagonal().prod()) / (2*np.pi)**(mean.size/2)\n",
        "\n",
        "# TODO: E-step\n",
        "def e_step(X, means, covariances, weights):\n",
        "    # Compute responsibilities\n",
        "    global K, N\n",
        "    r = np.zeros((N, K))\n",
        "    for i in range(K):\n",
        "        r[:, i] = weights[i] * gaussian_pdf(X, means[i], np.diag(covariances[i]))\n",
        "    r = r / r.sum(axis=1)[:, np.newaxis]\n",
        "    return r\n",
        "\n",
        "\n",
        "# TODO: M-step\n",
        "def m_step(X, r):\n",
        "    # Update means, covariances, weights\n",
        "    global N, K, D\n",
        "    global weights, means, covariances\n",
        "    weights = r.sum(axis=0)/N\n",
        "    means = (r / r.sum(axis=0)[np.newaxis, :]).T @ X\n",
        "    for i in range(K):\n",
        "        covariances[i] = np.diag(((X-means[i]).T @ ((r[:, i])[:, np.newaxis] * (X-means[i]))) / np.sum(r[:, i]))\n",
        "\n",
        "for i in range(1000):\n",
        "    r = e_step(X_scaled, means, covariances, weights)\n",
        "    m_step(X_scaled, r)\n",
        "\n",
        "cluster_labels_numpy = np.argmax(r, axis=1) # argmax of responsibilities"
      ],
      "metadata": {
        "id": "EclPwJbfvah5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4: Implementing GMM with EM **using PyTorch**\n",
        "\n",
        "Now, we will implement the same algorithm using PyTorch tensors. The steps are similar, but you will use `torch` operations. This might simplify certain operations and open the door to GPU acceleration.\n",
        "\n",
        "**Tasks:**  \n",
        "- Convert `X_scaled` to a PyTorch tensor.\n",
        "- Initialize parameters as `torch.tensor`s.\n",
        "- Implement E-step and M-step in PyTorch.\n",
        "- Run EM for a fixed number of iterations.\n",
        "- Extract cluster labels.\n",
        "\n",
        "**Hints:**\n",
        "- Use `torch.tensor(X_scaled, dtype=torch.float32)` to create a PyTorch tensor.\n",
        "- Operations are similar but use `torch.sum`, `torch.exp`, etc.\n",
        "- Watch out for broadcasting rules and ensure shapes align.\n"
      ],
      "metadata": {
        "id": "deQSDjycvmIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# TODO: Convert data to torch tensor\n",
        "X_torch = torch.tensor(X_scaled, dtype=torch.float32)\n",
        "\n",
        "# TODO: Initialize means, covariances, weights as torch tensors\n",
        "means_torch = torch.rand((K, D))\n",
        "covariances_torch = torch.ones((K, D))\n",
        "weights_torch = torch.rand(K)\n",
        "weights_torch /= weights_torch.sum()\n",
        "\n",
        "# TODO: Implement gaussian_pdf using torch operations\n",
        "def gaussian_pdf_torch(X, mean, cov):\n",
        "    P = (X-mean)**2 / torch.diag(cov) / 2\n",
        "    return torch.exp(-P.sum(axis=1)) / torch.sqrt(torch.diag(cov).prod()) / torch.pow(2*torch.pi, mean.prod()/2)\n",
        "\n",
        "# TODO: E-step in torch\n",
        "def e_step_torch(X, means, covariances, weights):\n",
        "    global K, N\n",
        "    r = torch.zeros((N, K))\n",
        "    for i in range(K):\n",
        "        r[:, i] = weights[i] * gaussian_pdf_torch(X, means[i], torch.diag(covariances[i]))\n",
        "    r = r / r.sum(dim=1, keepdim=True)\n",
        "    return r\n",
        "\n",
        "# TODO: M-step in torch\n",
        "def m_step_torch(X, r):\n",
        "    global N, K, D\n",
        "    global weights_torch, means_torch, covariances_torch\n",
        "    weights_torch = r.sum(axis=0)/N\n",
        "    means_torch = (r / r.sum(dim=0, keepdim=True)).T @ X\n",
        "    for i in range(K):\n",
        "        covariances_torch[i] = torch.diag(((X-means_torch[i]).T @ (r[:, i].unsqueeze(1) * (X-means_torch[i]))) / r[:, i].sum())\n",
        "\n",
        "\n",
        "# Run EM in torch\n",
        "for i in range(1000):\n",
        "    r = e_step_torch(X_torch, means_torch, covariances_torch, weights_torch)\n",
        "    m_step_torch(X_torch, r)\n",
        "\n",
        "cluster_labels_torch = torch.argmax(r, axis=1) # argmax of responsibilities\n"
      ],
      "metadata": {
        "id": "_-G1vfhGvi4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 5: Evaluating and Comparing Both Implementations\n",
        "\n",
        "**Tasks:**  \n",
        "- Use `adjusted_rand_score` to compare the cluster labels from both methods against the true labels `y`.\n",
        "- Print the ARI for both NumPy and PyTorch implementations.\n",
        "- Visually inspect if both implementations yield similar results.\n",
        "\n",
        "**Questions:**\n",
        "- Are the ARI scores similar or different between the two implementations?\n",
        "- Which code was easier to write and maintain?\n",
        "- Which implementation might be easier to extend to more complex models?\n"
      ],
      "metadata": {
        "id": "EcEETR0Hvsh0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import adjusted_rand_score\n",
        "\n",
        "# TODO: Compute ARI for numpy-based clustering\n",
        "ari_numpy = adjusted_rand_score(y, cluster_labels_numpy)\n",
        "print(\"ARI (NumPy):\", ari_numpy)\n",
        "\n",
        "# TODO: Compute ARI for torch-based clustering\n",
        "ari_torch = adjusted_rand_score(y, cluster_labels_torch)\n",
        "print(\"ARI (PyTorch):\", ari_torch)\n"
      ],
      "metadata": {
        "id": "xmumvOglvuJ1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c379efd4-6da9-4acd-f095-6d7990e29a19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ARI (NumPy): 0.8342589385759397\n",
            "ARI (PyTorch): 0.8022085453675192\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Questions:**  \n",
        "1. **Implementation Detail:** What are the main differences in code complexity between a plain NumPy-based implementation and a PyTorch-based one?  \n",
        "answer:\n",
        "\n",
        "2. **Performance:** Which implementation is likely to be more efficient or easier to parallelize and why?  \n",
        "answer:\n",
        "3. **Numerical Stability:** How might PyTorchâ€™s built-in functions improve numerical stability compared to a manual implementation?  \n",
        "answer:\n",
        "\n",
        "4. **Extendability:** If you wanted to add more complex features (e.g., full covariance matrices, regularization), which approach would be simpler and why?\n",
        "answer:"
      ],
      "metadata": {
        "id": "TMzyGBV1wP4D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.** NumPy requires more manual coding of math operations; PyTorch provides higher-level functions and automatic differentiation, making complex models easier to implement.\n",
        "\n",
        "**2.** PyTorch is generally more efficient due to GPU acceleration, automatic parallelization, and optimized operations.\n",
        "\n",
        "**3.** PyTorch offers built-in functions and optimized implementations that improve numerical stability compared to manual NumPy implementations.\n",
        "\n",
        "\n",
        "**4.** PyTorch's automatic differentiation and modular design make it much simpler to add complex features.\n"
      ],
      "metadata": {
        "id": "vlbpvk340Obz"
      }
    }
  ]
}